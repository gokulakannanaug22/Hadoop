1. Do the following 4 tasks for the sample data :

a. Create table and load HDFS data
b. Create table and load local data
c. Create external table and load HDFS data
d. Create external table and load local data

Use any sample file that was shared and do the above tasks and observe how the table and the file underneath behave.

2.
A. Create table and load as text file
-------------------------------------
1.
DROP TABLE twittble;

CREATE TABLE twittble(tweetId BIGINT, username STRING,txt STRING, CreatedAt STRING,     profileLocation STRING,favc BIGINT,retweet STRING,retcount BIGINT,followerscount BIGINT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

2.    
LOAD  DATA LOCAL INPATH  '/home/cloudera/inputfiles/TwitterData.txt' OVERWRITE INTO TABLE twittble;

3.
SELECT * FROM twittble;
! hadoop fs -ls /user/hive/warehouse/twittble;


B. Create a partition table:
-----------------------------
1.
DROP TABLE twitpart;

CREATE TABLE twitpart(tweetId BIGINT, username STRING, txt STRING,favc BIGINT,retweet STRING,retcount BIGINT,followerscount BIGINT) 
PARTITIONED BY(profileLocation STRING) 
ROW FORMAT DELIMITED FIELDS 
TERMINATED BY '\t' STORED AS TEXTFILE;

2.
INSERT OVERWRITE TABLE twitpart PARTITION (profileLocation="Chicago") 
SELECT tweetId,username,txt,favc,retweet,retcount,followerscount 
FROM twittble
WHERE profileLocation like '%Chicago%';

3.
select * from twitpart;
! hadoop fs -ls /user/hive/warehouse/twitpart;
! hadoop fs -ls /user/hive/warehouse/twitpart/profilelocation=Chicago;
! hadoop fs -cat /user/hive/warehouse/twitpart/profilelocation=Chicago/000000_0;


C. Create a dynamic partition table:
-------------------------------------
1.
DROP TABLE twitdynpart;

SET hive.exec.dynamic.partition.mode=nonstrict;
SET hive.exec.dynamic.partition=true;

CREATE TABLE twitdynpart(tweetId BIGINT,txt STRING,followerscount BIGINT) 
PARTITIONED BY(username STRING) 
ROW FORMAT DELIMITED FIELDS 
TERMINATED BY '\t' STORED AS TEXTFILE;

2.
INSERT OVERWRITE TABLE twitdynpart PARTITION (username) 
SELECT tweetId,txt,followerscount,username
FROM twittble
WHERE tweetId is not null;

3.
select * from twitdynpart;

show partitions twitdynpart;

! hadoop fs -ls /user/hive/warehouse/twitdynpart;
! hadoop fs -ls /user/hive/warehouse/twitdynpart/username=YoursTruly_Myy;
! hadoop fs -cat /user/hive/warehouse/twitdynpart/username=YoursTruly_Myy/000000_0;
Partitions for other usernames also created


D. Create a bucket table:
-------------------------
1.
DROP TABLE bucktbl;

CREATE TABLE bucktbl(tweetId BIGINT, username STRING,txt STRING,CreatedAt STRING,favc BIGINT,retweet STRING,retcount BIGINT, followerscount BIGINT)
PARTITIONED BY( profileLocation STRING)
CLUSTERED BY(tweetId) INTO 2 BUCKETS
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

2.
set hive.enforce.bucketing = true; 
INSERT OVERWRITE TABLE bucktbl PARTITION (profileLocation="Chicago")    
SELECT tweetId,username,txt,CreatedAt,favc,retweet,retcount,followerscount       
FROM twittble
where profileLocation like '%Chicago%';

3.
SELECT * FROM bucktbl;

! hadoop fs -ls /user/hive/warehouse/bucktbl;
! hadoop fs -ls /user/hive/warehouse/bucktbl/profilelocation=Chicago;
! hadoop fs -cat /user/hive/warehouse/bucktbl/profilelocation=Chicago/000000_0;
! hadoop fs -cat /user/hive/warehouse/bucktbl/profilelocation=Chicago/000001_0;


describe formatted twitpart;
describe formatted bucktbl;
describe extended bucktbl;


E. Create a Skew Table:
-----------------------
1.
SELECT * FROM twittble;

2.
set hive.mapred.supports.subdirectories=true;
set mapred.input.dir.recursive=true;

DROP TABLE twitskew;

CREATE TABLE  twitskew (tweetId BIGINT, username STRING,txt STRING,followerscount BIGINT)
SKEWED BY (tweetId) on (459917353988284416, 459917352285401088) 
STORED as DIRECTORIES;

3.
INSERT OVERWRITE TABLE twitskew     
SELECT tweetId,username,txt,followerscount       
FROM twittble
WHERE tweetId is not null;


4.
SELECT * FROM twitskew;
All the records will be displayed

! hadoop fs -ls /user/hive/warehouse/twitskew;
! hadoop fs -ls /user/hive/warehouse/twitskew/tweetid=459917353988284416;
! hadoop fs -ls /user/hive/warehouse/twitskew/tweetid=459917352285401088;
! hadoop fs -ls /user/hive/warehouse/twitskew/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME

sudo -i
hadoop fs -cat /user/hive/warehouse/twitskew/tweetid=459917353988284416/000000_0
hadoop fs -cat /user/hive/warehouse/twitskew/tweetid=459917352285401088/000000_0
hadoop fs -cat /user/hive/warehouse/twitskew/HIVE_DEFAULT_LIST_BUCKETING_DIR_NAME/000000_0

describe formatted twitskew;


F. Create table and load as ORC file
-------------------------------------
Optimized Row Columnar (ORC) File format is used as it further compresses data files. It could result in a small performance loss in writing, but there will be huge performance gain in reading.

1.
DROP TABLE twitorctbl;

CREATE TABLE twitorctbl(tweetId BIGINT, username STRING,txt STRING, CreatedAt STRING, profileLocation STRING COMMENT 'Location of user',favc INT,retweet STRING,retcount INT,followerscount INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY 't'
STORED AS ORC tblproperties ("orc.compress"="NONE");

2.
INSERT INTO TABLE twitorctbl SELECT * FROM twittble;

3.
SELECT * FROM twitorctbl;
! hadoop fs -ls /user/hive/warehouse/twitorctbl;
! hadoop fs -tail /user/hive/warehouse/twitorctbl/000000_0;



