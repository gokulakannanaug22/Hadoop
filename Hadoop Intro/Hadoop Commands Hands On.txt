Home directory of HDFS is always /user/username
so it is /user/focus2/

Help for all hadoop commands
-------------------------------
hadoop fs -help
hadoop fs -help ls


To get Hadoop version
---------------------
hadoop version


To list all the hadoop file system commands
-------------------------------------------
hadoop fs


To see all the files in a current directory
---------------------------------------------
hadoop fs -ls


To create a directory in 
---------------------------------------
hadoop fs -mkdir adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>


To delete a directory
---------------------
hadoop fs -rm -r adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>


To copy a file from Linux into HDFS . Use either put command or copyFromLocal
-------------------------------------------------------------------------------
create files a.txt and b.txt in Linux 

hadoop fs -put a.txt adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>

hadoop fs -copyFromLocal b.txt adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>



To display the data in the file
-------------------------------
hadoop fs -cat adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>


To list the files
------------------
hadoop fs -ls adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>



To copy a file from hdfs to Linux. Use either copyToLocal or get command
---------------------------------------------------------------------------
rm a.txt 

hadoop fs -copyToLocal adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>/a.txt /home/focus2/<<YourName>>/a.txt

hadoop fs -get adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>/a.txt /home/focus2/<<YourName>>


To remove the entire directory
------------------------------
hadoop fs -rm -r adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>/


To get the amount of space used and available on filesystem
-----------------------------------------------------------
hadoop fs -df hdfs:/


Ctrl C - To break a process
Exit - Exit from cloudera VM


To see all the commands we executed in VM
-------------------------------------------
History


To combine all the part files
----------------------------------
1. Create 2 files sample1.txt and sample2.txt

2. Place it in hdfs under <<YourName>> directory
hadoop fs -put sample1.txt adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>
hadoop fs -put sample2.txt adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>

3. Check the files in HDFS
hadoop fs -ls adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>

4. Merge the 2 files into single
hadoop fs -getmerge keerthi keerthisamplefile.txt
hadoop fs -getmerge adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>/sample1.txt adl://hadoopfocus.azuredatalakestore.net/clusters/focus2/<<YourName>>/sample2.txt samplefile.txt


cat samplefile.txt
All the part files under "<<YourName>>" directory will be combined and saved in samplefile.txt and moved to local file system




